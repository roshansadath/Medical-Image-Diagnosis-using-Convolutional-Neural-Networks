{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roshansadath/COMP6721-AppliedAI/blob/main/utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.models.inception import InceptionOutputs\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "WV88v66a3aYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_sampling(indices):\n",
        "  return torch.utils.data.sampler.SubsetRandomSampler(indices)"
      ],
      "metadata": {
        "id": "rzfbbPVRfHNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preprocessing\n",
        "def data_preprocess(data_path, sample_ratio):\n",
        "  # Create data transforms\n",
        "  data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "  # Get dataset from folder and apply data transforms\n",
        "  dataset = datasets.ImageFolder(root = \"{}data\".format(data_path), transform = data_transforms)\n",
        "    \n",
        "  # Get a sample of the data randomly\n",
        "  num_samples = int(len(dataset) * sample_ratio)\n",
        "  indices = np.random.choice(range(len(dataset)), num_samples, replace = False)\n",
        "\n",
        "  # Split the data into training, test, and validation sets\n",
        "  train_size = int(0.7 * num_samples)\n",
        "  test_size = int(0.2 * num_samples)\n",
        "  val_size = num_samples - train_size - test_size\n",
        "\n",
        "  train_indices = indices[ : train_size]\n",
        "  test_indices = indices[train_size : train_size + test_size]\n",
        "  val_indices = indices[train_size + test_size : ]\n",
        "\n",
        "  samples = [data_sampling(i) for i in [train_indices, test_indices, val_indices]]\n",
        "\n",
        "  # Create data loaders for training, test, and validation sets\n",
        "  train_loader = DataLoader(dataset, batch_size = batch_size, sampler = samples[0], num_workers = 4, pin_memory = True)\n",
        "  test_loader = DataLoader(dataset, batch_size = batch_size, sampler = samples[1], num_workers = 4, pin_memory = True)\n",
        "  val_loader = DataLoader(dataset, batch_size = batch_size, sampler = samples[2], num_workers = 4, pin_memory = True)\n",
        "\n",
        "  return dataset, train_loader, train_indices, test_loader, test_indices, val_loader, val_indices"
      ],
      "metadata": {
        "id": "sSFeMnrKeyo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, data_size, dtype, criterion, data_path, model_name):\n",
        "  _loss, _pred, _true, _accuracy = 0.0, [], [], []\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in dataloader:\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      _loss += loss.item() * inputs.size(0)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      _pred.extend(predicted.cpu().numpy())\n",
        "      _true.extend(labels.cpu().numpy())\n",
        "\n",
        "  _loss /= len(data_size)\n",
        "  _accuracy = accuracy_score(_true, _pred)  \n",
        "  _recall = recall_score(_true, _pred, average='macro')\n",
        "  _precision = precision_score(_true, _pred, average='macro')\n",
        "  _fscore = f1_score(_true, _pred, average='macro')\n",
        "\n",
        "  print('{}: Accuracy: {:.4f} | Loss: {:.4f} | Recall: {:.4f} | Precision: {:.4f} | F-score: {:.4f}'.format(dtype, _accuracy, _loss, _recall, _precision, _fscore))\n",
        "  print(\"\")\n",
        "\n",
        "  if(dtype == \"TEST\"):\n",
        "    cm = confusion_matrix(_true, _pred)\n",
        "    plt.figure(figsize = (8, 8))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = dataset.classes)\n",
        "    disp.plot()\n",
        "    plt.show()\n",
        "\n",
        "  else:\n",
        "    return _accuracy, _loss\n",
        "  \"\"\"  \n",
        "    plt.imshow(cm, cmap = plt.cm.Blues)\n",
        "    plt.title(\"{}_{}SET_CONFUSION_MATRIX\".format(model_name, dtype))\n",
        "    plt.colorbar()\n",
        "    plt.savefig(\"{}_{}SET_CONFUSION_MATRIX.png\".format(model_name, dtype))\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "gIfwoRhe1zF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, model_name, num_epochs):\n",
        "  losses, accuracies, true, pred, v_accuracies, v_losses = [], [], [], [], [], []\n",
        "  for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy, start_time = 0.0, 0.0, time.time()\n",
        "\n",
        "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as pbar:\n",
        "      for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs.logits if isinstance(outputs, InceptionOutputs) else outputs, dim = 1)\n",
        "        loss = criterion(outputs.logits if isinstance(outputs, InceptionOutputs) else outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss = loss.item() * inputs.size(0)\n",
        "        train_accuracy += torch.sum(preds == labels.data)\n",
        "        pred.extend(preds.cpu().numpy())\n",
        "        true.extend(labels.cpu().numpy())\n",
        "            \n",
        "        pbar.set_postfix({'Accuracy': train_accuracy.item()/len(train_indices), 'Loss': train_loss/len(train_indices)*100, 'Precision': precision_score(true, pred, average='macro'), 'Recall': recall_score(true, pred, average='macro'), 'F1 Score': f1_score(true, pred, average = 'macro')})\n",
        "        pbar.update()\n",
        "    \n",
        "    val_accuracy, val_loss = evaluate_model(model, val_loader, val_indices, 'VALIDATION', criterion, data_path, \"ResNet18\")\n",
        "\n",
        "    v_accuracies.append(val_accuracy)\n",
        "    v_losses.append(val_loss)\n",
        "    losses.append(train_loss/len(train_indices))\n",
        "    accuracies.append(train_accuracy.item()/len(train_indices))\n",
        "  \n",
        "  save_metrics(losses, accuracies, model_name)\n",
        "  return losses, accuracies, v_accuracies, v_losses"
      ],
      "metadata": {
        "id": "qPfNnVOIUbXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_TSNE(train_loader, device, model):\n",
        "  #Obtain the TSNE Plot for the data\n",
        "  features = []\n",
        "  labels = []\n",
        "  for images, targets in train_loader:\n",
        "      images = images.to(device)\n",
        "      targets = targets.to(device)\n",
        "      with torch.no_grad():\n",
        "          output = model(images)\n",
        "          features.append(output.cpu().numpy())\n",
        "          labels.append(targets.cpu().numpy())\n",
        "\n",
        "  features = np.vstack(features)\n",
        "  labels = np.concatenate(labels)\n",
        "\n",
        "  tsne = TSNE(n_components=2, perplexity = 25, learning_rate = 600, n_iter = 900)\n",
        "  tsne_features = tsne.fit_transform(features)\n",
        "\n",
        "  tsne_df = pd.DataFrame(data=tsne_features, columns=['t-SNE 1', 't-SNE 2'])\n",
        "  tsne_df['label'] = labels\n",
        "\n",
        "  # Plot the t-SNE plot with seaborn\n",
        "  sns.scatterplot(data=tsne_df, x='t-SNE 1', y='t-SNE 2', hue='label', palette='tab10')\n",
        "  plt.title('t-SNE Plot')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "pn4QipdVwfF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_within_class_variance(dataset):\n",
        "  #Get the class labels and the number of classes\n",
        "  class_labels = dataset.classes\n",
        "  num_classes = len(class_labels)\n",
        "\n",
        "  #Get the number of images per class\n",
        "  num_images_per_class = []\n",
        "  for i in range(num_classes):\n",
        "      class_indices = np.where(np.array(dataset.targets) == i)[0]\n",
        "      num_images_per_class.append(len(class_indices))\n",
        "\n",
        "  #Compute the mean and variance of the images per class\n",
        "  mean_num_images = np.mean(num_images_per_class)\n",
        "  var_num_images = np.var(num_images_per_class)\n",
        "\n",
        "  #Plot the within-class variance\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.bar(class_labels, num_images_per_class)\n",
        "  ax.axhline(y=mean_num_images, linestyle='--', color='r', label='Mean')\n",
        "  ax.axhspan(mean_num_images - np.sqrt(var_num_images), mean_num_images + np.sqrt(var_num_images),\n",
        "            alpha=0.2, color='y', label='Variance')\n",
        "  ax.legend()\n",
        "  plt.xticks(rotation = 0)\n",
        "  plt.ylabel('Number of Images')\n",
        "  plt.xlabel('Classes')\n",
        "  plt.title('Within-Class Variance Plot')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "q1V58AuLxZB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_model_curves():\n",
        "  #Plotting the Loss and Accuracy Curves\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "  ax1.plot(losses, label = \"Training Loss\")\n",
        "  ax1.plot(v_losses, label = \"Validation Loss\")\n",
        "  ax1.set_xlabel('Epoch')\n",
        "  ax1.set_ylabel('Loss')\n",
        "  ax1.set_title('Training and Validation Loss Curve')\n",
        "  ax1.legend()\n",
        "\n",
        "  ax2.plot(accuracies, label = \"Training Accuracy\")\n",
        "  ax2.plot(v_accuracies, label = \"Validation Accuracy\")\n",
        "  ax2.set_xlabel('Epoch')\n",
        "  ax2.set_ylabel('Accuracy')\n",
        "  ax2.set_title('Training and Validation Accuracy Curve')\n",
        "  ax2.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "iwHXAaTTyMk8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}